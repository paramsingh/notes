<h1 id="cross-entropy-loss">Cross Entropy Loss<a aria-hidden="true" class="anchor-heading icon-link" href="#cross-entropy-loss"></a></h1>
<ul>
<li>Cross-entropy loss works with multiple categories</li>
<li>Results in faster and more reliable training</li>
<li>default in fastai</li>
</ul>
<p>When we first take the softmax, and then the log likelihood of that,
that combination is called cross-entropy loss. In PyTorch, this is
available as <code>nn.CrossEntropyLoss</code> (which, in practice, actually
does log_softmax and then nll_loss).</p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/notes/ljq7kkmqkmnwu4e0fkfpate">Pet Breeds</a></li>
</ul>