<h1 id="lesson-6">Lesson 6<a aria-hidden="true" class="anchor-heading icon-link" href="#lesson-6"></a></h1>
<ul>
<li>
<p>gini - how likely is it that you go into the sample and grab two items, they're the same.</p>
</li>
<li>
<p>less processing for decision trees, no real dummy variables.</p>
</li>
<li>
<p>don't have to care about outliers, categorical variables</p>
</li>
<li>
<p>for tabular data, could start with decision trees for baseline approaches</p>
</li>
<li>
<p>really hard to mess it up.</p>
</li>
<li>
<p>can't split too deep because at some point the leaf nodes will not have much data.</p>
</li>
<li>
<p>bagging: take the average of a number of models.</p>
</li>
<li></li>
<li>
<p>find importances of the columns from random forests</p>
</li>
<li>
<p><a href="https://explained.ai/gradient-boosting/">https://explained.ai/gradient-boosting/</a></p>
</li>
</ul>
<ol>
<li>Create an effective validation set</li>
<li>Iterate rapidly to find changes which improve results on a validation set.</li>
</ol>
<p><code>dls.test_dl</code></p>